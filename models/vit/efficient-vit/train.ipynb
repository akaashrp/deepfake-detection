{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.6.15' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from einops import rearrange, repeat\n",
    "from torch import nn, einsum\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from random import random, randint, choice\n",
    "from vit_pytorch import ViT\n",
    "import numpy as np\n",
    "from torch.optim import lr_scheduler\n",
    "import os\n",
    "import json\n",
    "from os import cpu_count\n",
    "from multiprocessing.pool import Pool\n",
    "from functools import partial\n",
    "from multiprocessing import Manager\n",
    "from progress.bar import ChargingBar\n",
    "from efficient_vit import EfficientViT\n",
    "import uuid\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import cv2\n",
    "from transforms.albu import IsotropicResize\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from utils import train_parse, get_method, check_correct, resize, shuffle_dataset, get_n_params, train_val_test_split, video_to_frames\n",
    "from sklearn.utils.class_weight import compute_class_weight \n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import collections\n",
    "from deepfakes_dataset import DeepFakesDataset\n",
    "import math\n",
    "import yaml\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "BASE_DIR = '/global/cfs/projectdirs/m3641/Akaash/deepfake-detection/'\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "TRAINING_DIR = os.path.join(DATA_DIR, \"dfdc_train\")\n",
    "VALIDATION_DIR = os.path.join(DATA_DIR, \"dfdc_val\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test\")\n",
    "MODELS_PATH = os.path.join(BASE_DIR, \"models\")\n",
    "METADATA_PATH = os.path.join(DATA_DIR, \"metadata\") # Folder containing all training metadata for DFDC dataset\n",
    "TRAIN_LABELS_PATH = os.path.join(DATA_DIR, \"dfdc_train_labels.csv\")\n",
    "VALIDATION_LABELS_PATH = os.path.join(DATA_DIR, \"dfdc_val_labels.csv\")\n",
    "TEST_LABELS_PATH = os.path.join(DATA_DIR, \"dfdc_test_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.6.15' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def load_dataset(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def read_frames(video_path, train_dataset, validation_dataset):\n",
    "    # print(f\"Processing video {os.path.basename(video_path)}\")\n",
    "    if TRAINING_DIR in video_path:\n",
    "        train_df = pd.DataFrame(pd.read_csv(TRAIN_LABELS_PATH))\n",
    "        video_folder_name = os.path.basename(video_path)\n",
    "        video_key = video_folder_name + \".mp4\"\n",
    "        label = float(train_df.loc[train_df['filename'] == video_key]['label'].values[0])\n",
    "    else:\n",
    "        val_df = pd.DataFrame(pd.read_csv(VALIDATION_LABELS_PATH))\n",
    "        video_folder_name = os.path.basename(video_path)\n",
    "        video_key = video_folder_name + \".mp4\"\n",
    "        label = float(val_df.loc[val_df['filename'] == video_key]['label'].values[0])\n",
    "    \n",
    "    frames = os.listdir(video_path)\n",
    "    print(f\"Video {os.path.basename(video_path)}\", len(frames))\n",
    "        \n",
    "    for index, frame_image in enumerate(frames):\n",
    "        image = cv2.imread(os.path.join(video_path, frame_image))\n",
    "        if image is not None:\n",
    "            if TRAINING_DIR in video_path:\n",
    "                train_dataset.append((image, label))\n",
    "            else:\n",
    "                validation_dataset.append((image, label))\n",
    "\n",
    "def read_frames_wrapper(path, train_dataset, validation_dataset):\n",
    "    return read_frames(path, train_dataset, validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.6.15' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(BASE_DIR, \"models/vit/efficient-vit/configs/architecture.yaml\"), 'r') as ymlfile:\n",
    "    config = yaml.safe_load(ymlfile)\n",
    "\n",
    "channels = 1280\n",
    "\n",
    "print(\"EfficientNet B0 with ViT\")\n",
    "model = EfficientViT(config=config, channels=channels, selected_efficient_net = 0) # EfficientNet B0\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=config['training']['lr'], weight_decay=config['training']['weight-decay'])\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=config['training']['step-size'], gamma=config['training']['gamma'])\n",
    "starting_epoch = 0\n",
    "checkpoint_path = \"\"\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"Loading checkpoint from\", checkpoint_path)\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    starting_epoch = int(checkpoint_path.split(\"checkpoint\")[1].split(\"_\")[0]) + 1 # The checkpoint's file name format should be \"checkpoint_EPOCH\"\n",
    "else:\n",
    "    print(\"No checkpoint loaded.\")\n",
    "\n",
    "print(\"Model Parameters:\", get_n_params(model))\n",
    "\n",
    "# read dataset\n",
    "# if opt.dataset != \"All\" and opt.dataset != \"DFDC\":\n",
    "#     folders = [\"Original\", opt.dataset]\n",
    "# else:\n",
    "#     folders = [\"Original\", \"DFDC\", \"Deepfakes\", \"Face2Face\", \"FaceShifter\", \"FaceSwap\", \"NeuralTextures\"]\n",
    "\n",
    "mgr = Manager()\n",
    "if not os.path.exists(os.path.join(DATA_DIR, \"train.joblib\")) or not os.path.exists(os.path.join(DATA_DIR, \"val.joblib\")):\n",
    "    train_val_test_split(DATA_DIR, TRAINING_DIR, VALIDATION_DIR, TEST_DIR, 0.8, 0.1, 0.1)\n",
    "    sets = [TRAINING_DIR, VALIDATION_DIR]\n",
    "\n",
    "    print(\"Extracting frames from videos...\")\n",
    "    for dataset in sets:\n",
    "        videos = os.listdir(dataset)\n",
    "        videos = [video for video in videos if video.endswith(\".mp4\")]\n",
    "        for index, video in enumerate(videos):\n",
    "            if TRAINING_DIR in dataset and index == 100: # opt.max_train_videos\n",
    "                break\n",
    "            if VALIDATION_DIR in dataset and index == 10: # opt.max_val_videos\n",
    "                break\n",
    "            video_path = os.path.join(dataset, video)\n",
    "            frame_path = os.path.join(dataset, video.split(\".\")[0])\n",
    "            if not os.path.exists(frame_path):\n",
    "                video_to_frames(video_path, frame_path, frame_skip=10)\n",
    "    print(\"Frames extracted.\")\n",
    "\n",
    "    paths = []\n",
    "    for dataset in sets:\n",
    "        frame_folders = os.listdir(dataset)\n",
    "        frame_folders = [frame_folder for frame_folder in frame_folders if os.path.isdir(os.path.join(dataset, frame_folder))]\n",
    "        for index, frame_folder in enumerate(frame_folders):\n",
    "            if TRAINING_DIR in dataset and index == 99: # opt.max_train_videos\n",
    "                break\n",
    "            if VALIDATION_DIR in dataset and index == 10: # opt.max_val_videos\n",
    "                break\n",
    "\n",
    "            if os.path.isdir(os.path.join(dataset, frame_folder)):\n",
    "                paths.append(os.path.join(dataset, frame_folder))\n",
    "\n",
    "    # if len(paths) == 0:\n",
    "    #     paths = [TRAINING_DIR, VALIDATION_DIR]\n",
    "    \n",
    "    train_dataset = mgr.list()\n",
    "    validation_dataset = mgr.list()\n",
    "\n",
    "    # train_dataset = []\n",
    "    # validation_dataset = []\n",
    "\n",
    "    # with Pool(processes=56) as p: # opt.workers\n",
    "    #     with tqdm(total=len(paths)) as pbar:\n",
    "    #         for v in p.imap_unordered(partial(read_frames, train_dataset=train_dataset, validation_dataset=validation_dataset), paths):\n",
    "    #             pbar.update()\n",
    "\n",
    "    #     p.terminate()\n",
    "\n",
    "    # with tqdm(total=len(paths)) as pbar:\n",
    "    #     for path in paths:\n",
    "    #         result = read_frames(path, train_dataset=train_dataset, validation_dataset=validation_dataset)\n",
    "    #         pbar.update()\n",
    "        \n",
    "    #     pbar.close()\n",
    "    \n",
    "    with tqdm(total=len(paths)) as pbar:\n",
    "        with Parallel(n_jobs=56) as parallel:\n",
    "            results = parallel(delayed(read_frames_wrapper)(path, train_dataset, validation_dataset) for path in paths)\n",
    "            for _ in results:  # Update progress bar for each completed task\n",
    "                pbar.update()\n",
    "    \n",
    "    # train_dataset = shuffle_dataset(train_dataset)\n",
    "    # validation_dataset = shuffle_dataset(validation_dataset)\n",
    "    train_samples = len(train_dataset)\n",
    "    validation_samples = len(validation_dataset)\n",
    "\n",
    "    # with open(os.path.join(DATA_DIR, \"train.joblib\"), \"wb\") as f:\n",
    "    #     joblib.dump(train_dataset, f, compress=1)\n",
    "    # with open(os.path.join(DATA_DIR, \"val.joblib\"), \"wb\") as f:\n",
    "    #     joblib.dump(validation_dataset, f, compress=1)\n",
    "else:\n",
    "    # with open(os.path.join(DATA_DIR, \"train.joblib\"), \"rb\") as f:\n",
    "    #     train_dataset = joblib.load(f)\n",
    "    # with open(os.path.join(DATA_DIR, \"val.joblib\"), \"rb\") as f:\n",
    "    #     validation_dataset = joblib.load(f)\n",
    "\n",
    "    train_dataset_getter = mgr.Function(load_dataset, [DATA_DIR + \"/train.pkl\"])\n",
    "    train_dataset = train_dataset_getter()\n",
    "    validation_dataset_getter = mgr.Function(load_dataset, [DATA_DIR + \"/val.pkl\"])\n",
    "    validation_dataset = validation_dataset_getter()\n",
    "    \n",
    "    train_samples = len(train_dataset)\n",
    "    validation_samples = len(validation_dataset)\n",
    "\n",
    "# Print some useful statistics\n",
    "print(\"Train images:\", train_samples, \"Validation images:\", validation_samples)\n",
    "print(\"__TRAINING STATS__\")\n",
    "train_counters = collections.Counter(image[1] for image in train_dataset)\n",
    "print(train_counters)\n",
    "\n",
    "class_weights = train_counters[0] / train_counters[1]\n",
    "print(\"Weights\", class_weights)\n",
    "\n",
    "print(\"__VALIDATION STATS__\")\n",
    "val_counters = collections.Counter(image[1] for image in validation_dataset)\n",
    "print(val_counters)\n",
    "\n",
    "print(\"___________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.6.15' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# with open(os.path.join(DATA_DIR, \"train.joblib\"), \"rb\") as f:\n",
    "#     train_dataset = joblib.load(f)\n",
    "# with open(os.path.join(DATA_DIR, \"val.joblib\"), \"rb\") as f:\n",
    "#     validation_dataset = joblib.load(f)\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([class_weights])) # log loss\n",
    "\n",
    "# print(train_dataset[0])\n",
    "print(train_dataset[0][0].shape)\n",
    "\n",
    "# Create the data loaders\n",
    "validation_labels = np.asarray([row[1] for row in validation_dataset])\n",
    "labels = np.asarray([row[1] for row in train_dataset])\n",
    "\n",
    "train_image_length = 3001\n",
    "train_images = np.asarray([\n",
    "    row[0] if len(row[0]) == train_image_length else np.zeros(train_image_length)  # Pad with zeros\n",
    "    for row in train_dataset\n",
    "])\n",
    "\n",
    "val_image_length = 600\n",
    "val_images = np.asarray([\n",
    "    row[0] if len(row[0]) == val_image_length else np.zeros(val_image_length)  # Pad with zeros\n",
    "    for row in train_dataset\n",
    "])\n",
    "\n",
    "train_dataset = DeepFakesDataset([row[0] for row in train_dataset], labels, config['model']['image-size'])\n",
    "dl = torch.utils.data.DataLoader(train_dataset, batch_size=config['training']['bs'], shuffle=True, sampler=None,\n",
    "                                batch_sampler=None, num_workers=1, collate_fn=None,\n",
    "                                pin_memory=False, drop_last=False, timeout=0,\n",
    "                                worker_init_fn=None, prefetch_factor=2,\n",
    "                                persistent_workers=False)\n",
    "del train_dataset\n",
    "\n",
    "validation_dataset = DeepFakesDataset([row[0] for row in validation_dataset], validation_labels, config['model']['image-size'], mode='validation')\n",
    "val_dl = torch.utils.data.DataLoader(validation_dataset, batch_size=config['training']['bs'], shuffle=True, sampler=None,\n",
    "                                batch_sampler=None, num_workers=1, collate_fn=None,\n",
    "                                pin_memory=False, drop_last=False, timeout=0,\n",
    "                                worker_init_fn=None, prefetch_factor=2,\n",
    "                                persistent_workers=False)\n",
    "del validation_dataset\n",
    "\n",
    "print(\"Beginning training...\")\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "counter = 0\n",
    "not_improved_loss = 0\n",
    "previous_loss = math.inf\n",
    "for t in range(starting_epoch, 300 + 1): # opt.num_epochs\n",
    "    if not_improved_loss == config[\"training\"][\"patience\"]:\n",
    "        break\n",
    "    counter = 0\n",
    "\n",
    "    total_loss = 0\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    bar = ChargingBar('EPOCH #' + str(t), max=(len(dl)*config['training']['bs'])+len(val_dl))\n",
    "    train_correct = 0\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    for index, (images, labels) in enumerate(dl):\n",
    "        images = np.transpose(images, (0, 3, 1, 2))\n",
    "        labels = labels.unsqueeze(1)\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "        \n",
    "        y_pred = model(images)\n",
    "        y_pred = y_pred.cpu()\n",
    "        \n",
    "        loss = loss_fn(y_pred, labels)\n",
    "    \n",
    "        corrects, positive_class, negative_class = check_correct(y_pred, labels)  \n",
    "        train_correct += corrects\n",
    "        positive += positive_class\n",
    "        negative += negative_class\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        counter += 1\n",
    "        total_loss += round(loss.item(), 2)\n",
    "        \n",
    "        if index % 1200 == 0: # Intermediate metrics print\n",
    "            print(\"\\nLoss: \", total_loss/counter, \"Accuracy: \",train_correct/(counter*config['training']['bs']) ,\"Train 0s: \", negative, \"Train 1s:\", positive)\n",
    "\n",
    "        for i in range(config['training']['bs']):\n",
    "            bar.next()\n",
    "\n",
    "    val_correct = 0\n",
    "    val_positive = 0\n",
    "    val_negative = 0\n",
    "    val_counter = 0\n",
    "    train_correct /= train_samples\n",
    "    total_loss /= counter\n",
    "    for index, (val_images, val_labels) in enumerate(val_dl):\n",
    "        val_images = np.transpose(val_images, (0, 3, 1, 2))\n",
    "        if torch.cuda.is_available():\n",
    "            val_images = val_images.cuda()\n",
    "        val_labels = val_labels.unsqueeze(1)\n",
    "        val_pred = model(val_images)\n",
    "        val_pred = val_pred.cpu()\n",
    "        val_loss = loss_fn(val_pred, val_labels)\n",
    "        total_val_loss += round(val_loss.item(), 2)\n",
    "        corrects, positive_class, negative_class = check_correct(val_pred, val_labels)\n",
    "        val_correct += corrects\n",
    "        val_positive += positive_class\n",
    "        val_counter += 1\n",
    "        val_negative += negative_class\n",
    "        bar.next()\n",
    "        \n",
    "    scheduler.step()\n",
    "    bar.finish()\n",
    "    \n",
    "    total_val_loss /= val_counter\n",
    "    val_correct /= validation_samples\n",
    "    if previous_loss <= total_val_loss:\n",
    "        print(\"Validation loss did not improve\")\n",
    "        not_improved_loss += 1\n",
    "    else:\n",
    "        not_improved_loss = 0\n",
    "    \n",
    "    previous_loss = total_val_loss\n",
    "    # opt.num_epochs\n",
    "    print(\"#\" + str(t) + \"/\" + str(300) + \" loss:\" +\n",
    "        str(total_loss) + \" accuracy:\" + str(train_correct) +\" val_loss:\" + str(total_val_loss) + \" val_accuracy:\" + str(val_correct) + \" val_0s:\" + str(val_negative) + \"/\" + str(np.count_nonzero(validation_labels == 0)) + \" val_1s:\" + str(val_positive) + \"/\" + str(np.count_nonzero(validation_labels == 1)))\n",
    "\n",
    "    if not os.path.exists(MODELS_PATH):\n",
    "        os.makedirs(MODELS_PATH)\n",
    "    if t % config[\"training\"][\"save-frequency\"] == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(MODELS_PATH, \"efficientnetB0_checkpoint\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
